{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and test set with 500 examples of classes 5 and 8 respectively and change labels to 0 and 1. At the same time flip 30% of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [ex for ex in train_dataset if ex[1]==5][:500] + [ex for ex in train_dataset if ex[1]==8][:500]\n",
    "test_set = [ex for ex in test_dataset if ex[1]==5][:500] + [ex for ex in test_dataset if ex[1]==8][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_01_images = []\n",
    "test_set_01_images = []\n",
    "train_set_01_labels = []\n",
    "test_set_01_labels = []\n",
    "num_flips = 500//3 #num_flips=0 to have no flips\n",
    "for i,ex_tr in enumerate(train_set): # [0,499] ->5, [500,999] ->8\n",
    "    ex_tr = list(ex_tr)\n",
    "    if ex_tr[1]==5:\n",
    "        if i<num_flips: # 0-30% of 5s(0-499) are flipped to 8s, \n",
    "            ex_tr[1]=1\n",
    "            # 1 is the label for 8s\n",
    "            # 0 is the label for 5s\n",
    "        else: \n",
    "            ex_tr[1]=0\n",
    "    else: \n",
    "        if i<num_flips+500: # 0-30% of 8s(500-999) are flipped to 5s\n",
    "            ex_tr[1]=0\n",
    "        else: \n",
    "            ex_tr[1]=1\n",
    "    train_set_01_images.append(ex_tr[0])\n",
    "    train_set_01_labels.append(ex_tr[1])\n",
    "    \n",
    "for ex_te in test_set:\n",
    "    ex_te = list(ex_te)\n",
    "    if ex_te[1]==5:\n",
    "        ex_te[1]=0\n",
    "    else: \n",
    "        ex_te[1]=1\n",
    "    test_set_01_images.append(ex_te[0])\n",
    "    test_set_01_labels.append(ex_te[1])\n",
    "train_set_01_images = torch.stack(train_set_01_images)  # why stack? anwser: to make it a tensor, and the shape is (1000, 1, 28, 28)\n",
    "train_set_01_images = train_set_01_images.view(-1, 28*28) # 1000, 784\n",
    "\n",
    "test_set_01_images = torch.stack(test_set_01_images)\n",
    "test_set_01_images = test_set_01_images.view(-1, 28*28)\n",
    "\n",
    "train_set_01_labels = torch.tensor(train_set_01_labels)\n",
    "test_set_01_labels = torch.tensor(test_set_01_labels) \n",
    "\n",
    "# shuffle training set\n",
    "n_sample = len(train_set_01_images)\n",
    "np.random.seed(0)\n",
    "order = np.random.permutation(n_sample)\n",
    "train_set_01_images = train_set_01_images[order]\n",
    "train_set_01_labels = train_set_01_labels[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size train set: torch.Size([1000, 784])\n",
      "size test set: torch.Size([1000, 784])\n"
     ]
    }
   ],
   "source": [
    "print(\"size train set:\", train_set_01_images.shape)\n",
    "print(\"size test set:\", test_set_01_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(X):\n",
    "#     return 1/(1+torch.exp(-X))\n",
    "\n",
    "# def f(X,theta): # x 1*784, theta 784*1\n",
    "#     return sigmoid(torch.matmul(X,theta))\n",
    "\n",
    "# def loss(X,y,theta):\n",
    "#     epsilon = 1e-8 # to avoid nan\n",
    "#     X = X.type(torch.float)\n",
    "#     y = y.type(torch.float)\n",
    "#     loss1 = -torch.matmul(y,torch.log(f(X,theta)+epsilon)) - torch.matmul((1-y),torch.log(1-f(X,theta)+epsilon))\n",
    "#     return loss1\n",
    "\n",
    "# def loss_grad(X,y,theta):\n",
    "#     X = X.type(torch.float)\n",
    "#     y = y.type(torch.float)\n",
    "#     return torch.matmul(X.T, f(X,theta)-y).sum(1).view(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_epochs = 10000\n",
    "# step_size = 0.0000001\n",
    "# theta1 = torch.ones(28*28,1)\n",
    "\n",
    "# for i in range(max_epochs): \n",
    "#     grad = loss_grad(train_set_01_images, train_set_01_labels, theta1)\n",
    "#     tmp = theta1\n",
    "#     theta1 = theta1 - step_size * grad\n",
    "#     print(loss(train_set_01_images, train_set_01_labels, theta1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6960861682891846\n",
      "0.6947778463363647\n",
      "0.6925340890884399\n",
      "0.6897932887077332\n",
      "0.686898410320282\n",
      "0.6840081214904785\n",
      "0.6811043620109558\n",
      "0.678074836730957\n",
      "0.6748166680335999\n",
      "0.6713123917579651\n",
      "0.6676486134529114\n",
      "0.6639828085899353\n",
      "0.6604799628257751\n",
      "0.6572562456130981\n",
      "0.6543488502502441\n",
      "0.6517247557640076\n",
      "0.6493131518363953\n",
      "0.6470455527305603\n",
      "0.6448848247528076\n",
      "0.6428319811820984\n",
      "0.6409149169921875\n",
      "0.6391669511795044\n",
      "0.6376073956489563\n",
      "0.6362307667732239\n",
      "0.6350095272064209\n",
      "0.6339043378829956\n",
      "0.63287752866745\n",
      "0.6319031715393066\n",
      "0.6309708952903748\n",
      "0.6300820708274841\n",
      "0.629244327545166\n",
      "0.6284630298614502\n",
      "0.6277372241020203\n",
      "0.6270591616630554\n",
      "0.6264159679412842\n",
      "0.6257942914962769\n",
      "0.6251840591430664\n",
      "0.6245806813240051\n",
      "0.6239843368530273\n",
      "0.62339848279953\n",
      "0.6228272318840027\n",
      "0.6222730875015259\n",
      "0.6217358112335205\n",
      "0.6212129592895508\n",
      "0.6207011938095093\n",
      "0.6201969981193542\n",
      "0.6196988821029663\n",
      "0.6192066073417664\n",
      "0.6187218427658081\n",
      "0.6182464361190796\n",
      "0.6177821755409241\n",
      "0.6173298358917236\n",
      "0.6168890595436096\n",
      "0.6164587736129761\n",
      "0.6160378456115723\n",
      "0.615625262260437\n",
      "0.6152204871177673\n",
      "0.6148234605789185\n",
      "0.6144348978996277\n",
      "0.6140550374984741\n",
      "0.6136842966079712\n",
      "0.6133224964141846\n",
      "0.612969160079956\n",
      "0.6126235723495483\n",
      "0.6122848987579346\n",
      "0.6119529008865356\n",
      "0.6116268634796143\n",
      "0.6113070845603943\n",
      "0.6109932661056519\n",
      "0.6106854677200317\n",
      "0.6103835701942444\n",
      "0.6100872755050659\n",
      "0.6097962260246277\n",
      "0.609510064125061\n",
      "0.6092284321784973\n",
      "0.6089510917663574\n",
      "0.608677864074707\n",
      "0.6084085702896118\n",
      "0.6081433892250061\n",
      "0.6078819036483765\n",
      "0.6076242923736572\n",
      "0.6073701977729797\n",
      "0.6071195006370544\n",
      "0.6068720817565918\n",
      "0.6066278219223022\n",
      "0.6063864827156067\n",
      "0.6061481833457947\n",
      "0.6059126257896423\n",
      "0.605679988861084\n",
      "0.6054500937461853\n",
      "0.6052228808403015\n",
      "0.6049982905387878\n",
      "0.6047762632369995\n",
      "0.6045566201210022\n",
      "0.6043394207954407\n",
      "0.6041243672370911\n",
      "0.603911817073822\n",
      "0.6037013530731201\n",
      "0.6034931540489197\n",
      "0.6032870411872864\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "max_epochs = 1000\n",
    "X = train_set_01_images.type(torch.float)\n",
    "y = train_set_01_labels.type(torch.float).view(-1,1)\n",
    "for epochs in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1,1), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
