{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and test set with 500 examples of classes 5 and 8 respectively and change labels to 0 and 1. At the same time flip 30% of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [ex for ex in train_dataset if ex[1]==5][:500] + [ex for ex in train_dataset if ex[1]==8][:500]\n",
    "test_set = [ex for ex in test_dataset if ex[1]==5][:500] + [ex for ex in test_dataset if ex[1]==8][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_01_images = []\n",
    "test_set_01_images = []\n",
    "train_set_01_labels = []\n",
    "test_set_01_labels = []\n",
    "num_flips = 500//3 #num_flips=0 to have no flips\n",
    "for i,ex_tr in enumerate(train_set): # [0,499] ->5, [500,999] ->8\n",
    "    ex_tr = list(ex_tr)\n",
    "    if ex_tr[1]==5:\n",
    "        if i<num_flips: # 0-30% of 5s(0-499) are flipped to 8s, \n",
    "            ex_tr[1]=1\n",
    "            # 1 is the label for 8s\n",
    "            # 0 is the label for 5s\n",
    "        else: \n",
    "            ex_tr[1]=0\n",
    "    else: \n",
    "        if i<num_flips+500: # 0-30% of 8s(500-999) are flipped to 5s\n",
    "            ex_tr[1]=0\n",
    "        else: \n",
    "            ex_tr[1]=1\n",
    "    train_set_01_images.append(ex_tr[0])\n",
    "    train_set_01_labels.append(ex_tr[1])\n",
    "    \n",
    "for ex_te in test_set:\n",
    "    ex_te = list(ex_te)\n",
    "    if ex_te[1]==5:\n",
    "        ex_te[1]=0\n",
    "    else: \n",
    "        ex_te[1]=1\n",
    "    test_set_01_images.append(ex_te[0])\n",
    "    test_set_01_labels.append(ex_te[1])\n",
    "train_set_01_images = torch.stack(train_set_01_images)  # why stack? anwser: to make it a tensor, and the shape is (1000, 1, 28, 28)\n",
    "train_set_01_images = train_set_01_images.view(-1, 28*28) # 1000, 784\n",
    "\n",
    "test_set_01_images = torch.stack(test_set_01_images)\n",
    "test_set_01_images = test_set_01_images.view(-1, 28*28)\n",
    "\n",
    "train_set_01_labels = torch.tensor(train_set_01_labels)\n",
    "test_set_01_labels = torch.tensor(test_set_01_labels) \n",
    "\n",
    "# shuffle training set\n",
    "n_sample = len(train_set_01_images)\n",
    "np.random.seed(0)\n",
    "order = np.random.permutation(n_sample)\n",
    "train_set_01_images = train_set_01_images[order]\n",
    "train_set_01_labels = train_set_01_labels[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size train set: torch.Size([1000, 784])\n",
      "size test set: torch.Size([1000, 784])\n"
     ]
    }
   ],
   "source": [
    "print(\"size train set:\", train_set_01_images.shape)\n",
    "print(\"size test set:\", test_set_01_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(X):\n",
    "#     return 1/(1+torch.exp(-X))\n",
    "\n",
    "# def f(X,theta): # x 1*784, theta 784*1\n",
    "#     return sigmoid(torch.matmul(X,theta))\n",
    "\n",
    "# def loss(X,y,theta):\n",
    "#     epsilon = 1e-8 # to avoid nan\n",
    "#     X = X.type(torch.float)\n",
    "#     y = y.type(torch.float)\n",
    "#     loss1 = -torch.matmul(y,torch.log(f(X,theta)+epsilon)) - torch.matmul((1-y),torch.log(1-f(X,theta)+epsilon))\n",
    "#     return loss1\n",
    "\n",
    "# def loss_grad(X,y,theta):\n",
    "#     X = X.type(torch.float)\n",
    "#     y = y.type(torch.float)\n",
    "#     return torch.matmul(X.T, f(X,theta)-y).sum(1).view(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_epochs = 10000\n",
    "# step_size = 0.0000001\n",
    "# theta1 = torch.ones(28*28,1)\n",
    "\n",
    "# for i in range(max_epochs): \n",
    "#     grad = loss_grad(train_set_01_images, train_set_01_labels, theta1)\n",
    "#     tmp = theta1\n",
    "#     theta1 = theta1 - step_size * grad\n",
    "#     print(loss(train_set_01_images, train_set_01_labels, theta1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.690821647644043\n",
      "0.6900579929351807\n",
      "0.6886357665061951\n",
      "0.6866686940193176\n",
      "0.6842674612998962\n",
      "0.6815328598022461\n",
      "0.6785529851913452\n",
      "0.6754049062728882\n",
      "0.6721571683883667\n",
      "0.6688730716705322\n",
      "0.665610134601593\n",
      "0.6624193787574768\n",
      "0.6593431234359741\n",
      "0.656412661075592\n",
      "0.653647780418396\n",
      "0.6510586738586426\n",
      "0.6486475467681885\n",
      "0.6464121341705322\n",
      "0.6443473100662231\n",
      "0.6424461603164673\n",
      "0.640701413154602\n",
      "0.6391039490699768\n",
      "0.6376432776451111\n",
      "0.6363075375556946\n",
      "0.6350839138031006\n",
      "0.6339593529701233\n",
      "0.6329211592674255\n",
      "0.6319580674171448\n",
      "0.6310603022575378\n",
      "0.6302192211151123\n",
      "0.6294276118278503\n",
      "0.6286789774894714\n",
      "0.6279674172401428\n",
      "0.6272876262664795\n",
      "0.6266348361968994\n",
      "0.626004695892334\n",
      "0.6253939867019653\n",
      "0.6248000264167786\n",
      "0.6242208480834961\n",
      "0.6236551403999329\n",
      "0.6231017112731934\n",
      "0.6225597858428955\n",
      "0.6220288872718811\n",
      "0.6215083599090576\n",
      "0.6209975481033325\n",
      "0.6204963326454163\n",
      "0.620004415512085\n",
      "0.6195217967033386\n",
      "0.6190482378005981\n",
      "0.6185840368270874\n",
      "0.6181291937828064\n",
      "0.6176837086677551\n",
      "0.617247462272644\n",
      "0.6168205142021179\n",
      "0.6164025068283081\n",
      "0.6159934401512146\n",
      "0.6155930161476135\n",
      "0.6152013540267944\n",
      "0.6148179769515991\n",
      "0.6144428849220276\n",
      "0.6140758395195007\n",
      "0.6137165427207947\n",
      "0.6133649349212646\n",
      "0.6130204200744629\n",
      "0.6126829981803894\n",
      "0.612352192401886\n",
      "0.6120278239250183\n",
      "0.611709713935852\n",
      "0.611397385597229\n",
      "0.611090898513794\n",
      "0.6107897758483887\n",
      "0.6104938387870789\n",
      "0.6102029085159302\n",
      "0.6099168658256531\n",
      "0.6096352338790894\n",
      "0.609358012676239\n",
      "0.6090850830078125\n",
      "0.6088161468505859\n",
      "0.6085511445999146\n",
      "0.6082897782325745\n",
      "0.6080321669578552\n",
      "0.6077779531478882\n",
      "0.6075270771980286\n",
      "0.6072795391082764\n",
      "0.6070351600646973\n",
      "0.6067938208580017\n",
      "0.6065554022789001\n",
      "0.6063199043273926\n",
      "0.6060871481895447\n",
      "0.6058573126792908\n",
      "0.6056299209594727\n",
      "0.6054052710533142\n",
      "0.6051830649375916\n",
      "0.6049633026123047\n",
      "0.6047460436820984\n",
      "0.6045309901237488\n",
      "0.604318380355835\n",
      "0.6041079163551331\n",
      "0.6038996577262878\n",
      "0.6036935448646545\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "max_epochs = 1000\n",
    "X = train_set_01_images.type(torch.float)\n",
    "y = train_set_01_labels.type(torch.float).view(-1,1)\n",
    "for epochs in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1,1), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
